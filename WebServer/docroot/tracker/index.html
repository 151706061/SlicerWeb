<!DOCTYPE html>
<html>
<head>

  <title>Browser tracking for mobile</title>

  <link rel="stylesheet" href="../stylesheets/application.css" />
  <script src='http://code.jquery.com/jquery-2.0.3.min.js'></script>

</head>

<body>


<script> /* getSources inspired from https://github.com/samdutton/simpl/tree/master/getusermedia */ </script>

<h1>Data for tracking</h1>

<h2>Be sure to approve requests for the camera(s) and microphone(s).</h2>

<img src="../images/apple-keyboard.jpg" height=300></img>

<span id="graphics"></span>
<div id="canvases"></div>

<script>
//
// This is the graphics part.
// This is responsible for grabbing video frames as fast as possible
// from the video part and putting them in texture.
// Then calculating an estimate of the camera offset that generated the frames.
//

// first, the shaders
</script>

<script id="vertexShader" type="x-shader/x-vertex">

uniform vec2 dimensions;

attribute vec4 position;

varying vec2 textureCoordinate;

void main(void) {
  vec4 newPosition = position;
  newPosition.x += .0;
  gl_Position = newPosition;
  textureCoordinate.x = position.x;
  textureCoordinate.y = position.y * (dimensions.y / dimensions.x);
}
</script>

<script id="fragmentShader" type="x-shader/x-fragment">

#ifdef GL_ES
precision highp float;
#endif

uniform sampler2D tex0;

varying vec2 textureCoordinate;

void main(void) {

  vec4 rgba = texture2D(tex0, textureCoordinate);

  gl_FragColor = rgba;
}
</script>

<script>
//
// Now, the rendering code
//

// add a canvas for 3D
var canvasID = "canvas3D";
$('#canvases').append('<canvas id=canvas3D></canvas><p>Rendered View</p>');
var canvas3D = document.querySelector('#canvas3D');
</script>

<script>
//
// set up webgl
//
var gl = canvas3D.getContext('webgl');
gl.clearColor(0.0, 0.0, 0.0, 1.0); // black, fully opaque
gl.enable(gl.DEPTH_TEST);
gl.depthFunc(gl.LEQUAL); // Near things obscure far things

// create the vertex buffer
var squareVertices = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, squareVertices);
var vertices = [
   1.0,  1.0,  0.0,
  -1.0,  1.0,  0.0,
   1.0, -1.0,  0.0,
  -1.0, -1.0,  0.0
];
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(vertices), gl.STATIC_DRAW);

// create the texture
var textureImage = new Image();
var texture = gl.createTexture();
var loadTexture = function() {
  gl.bindTexture(gl.TEXTURE_2D, texture);
  gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, textureImage);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
  gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
  gl.bindTexture(gl.TEXTURE_2D, null);
  render3D(); // TODO: move this to requestAnimationFrame
};



//
// create the program and shaders
//
var glProgram = gl.createProgram();
var vertexShader = gl.createShader(gl.VERTEX_SHADER);
gl.shaderSource(vertexShader, document.getElementById("vertexShader").innerHTML);
gl.compileShader(vertexShader);
if (!gl.getShaderParameter(vertexShader, gl.COMPILE_STATUS)) {
  alert('Could not compile vertexShader');
}
var fragmentShader = gl.createShader(gl.FRAGMENT_SHADER);
gl.shaderSource(fragmentShader, document.getElementById("fragmentShader").innerHTML);
gl.compileShader(fragmentShader);
if (!gl.getShaderParameter(fragmentShader, gl.COMPILE_STATUS)) {
  alert('Could not compile fragmentShader');
}
gl.attachShader(glProgram, vertexShader);
gl.deleteShader(vertexShader);
gl.attachShader(glProgram, fragmentShader);
gl.deleteShader(fragmentShader);
gl.linkProgram(glProgram);

gl.enableVertexAttribArray( gl.getAttribLocation(glProgram, "position") );

var dimensions = [200.,300.];

function render3D() {
  gl.viewport(0, 0, canvas3D.width, canvas3D.height);
  gl.clear(gl.COLOR_BUFFER_BIT|gl.DEPTH_BUFFER_BIT);

  gl.useProgram(glProgram);

  // the position attribute
  var positionLocation = gl.getAttribLocation(glProgram, "position");
  gl.vertexAttribPointer( positionLocation, 3, gl.FLOAT, false, 0, 0);

  // the texture
  gl.activeTexture(gl.TEXTURE0);
  gl.bindTexture(gl.TEXTURE_2D, texture);
  gl.uniform1i(gl.getUniformLocation(glProgram, "tex0"), 0);

  // dimensions
  gl.uniform2f(gl.getUniformLocation(glProgram, "dimensions"), dimensions[0], dimensions[1]);

  gl.bindBuffer(gl.ARRAY_BUFFER, squareVertices);
  gl.drawArrays(gl.TRIANGLE_STRIP, 0, 4);
}

//textureImage.src = "../images/3DSlicer-DesktopIcon.png";
textureImage.src = "../images/apple-keyboard.jpg";
textureImage.onload = loadTexture;

</script>


<span id="video"></span>
<div id='videos'> </div>
<div id='videoTime'> </div>
<script>
//
// This is the video part - it is responsible for collecting
// frames and putting them into a canvas.
//

'use strict';

var videoLabel = document.getElementById('video');
videoLabel.innerHTML = "<p> waiting for video events...</p>";


// handle browser differences
navigator.getUserMedia = navigator.getUserMedia ||
  navigator.webkitGetUserMedia || navigator.mozGetUserMedia;

// seems only one video stream can be active at a time, at least
// on chrome on android so we pick the second video source
var skipFirstVideo = true;

// when sources are found
function gotSources(sourceInfos) {
  for (var i = 0; i !== sourceInfos.length; ++i) {
    var sourceInfo = sourceInfos[i];
    if (sourceInfo.kind === 'video') {
      if (!skipFirstVideo) {
        videoLabel.innerHTML = "<p> Found some video </p>";
        videoStart(sourceInfo.id);
      }
      skipFirstVideo = false;
    }
    if (sourceInfo.kind === 'audio') {
      console.log("There's audio available, but we don't care");
    } else {
      console.log('Some other kind of source: ', sourceInfo);
    }
  }
}

// ask for sources
if (typeof MediaStreamTrack.getSources === 'undefined'){
  alert('This browser does not support MediaStreamTrack.\n\nTry Chrome Canary.');
} else {
  MediaStreamTrack.getSources(gotSources);
}

var videoCount = 0;
// play the video when it's available
function videoSuccessCallback(stream) {
  videoCount += 1;
  var videoID = "video"+videoCount;
  $('#videos').append('<video autoplay id='+videoID+'></video><p>'+videoID+'</p>');
  var videoElement = document.querySelector('#video1');
  videoElement.src = window.URL.createObjectURL(stream);
  videoElement.addEventListener('canplay', function(ev){
    var width = 200;
    var height = videoElement.videoHeight / (videoElement.videoWidth/width);
    videoElement.setAttribute('width', width);
    videoElement.setAttribute('height', height);
    canvas3D.setAttribute('width', width);
    canvas3D.setAttribute('height', height);
    dimensions[0] = width;
    dimensions[1] = height;

    render3D(); // initial render

  }, false);
  var videoTime = document.getElementById('videoTime');
  videoElement.addEventListener('timeupdate', function() {
    videoTime.innerHTML = "<p>Time: " + videoElement.currentTime + "</p>";
  });
  videoElement.play();
}

// or error out
function errorCallback(error){
  console.log('navigator.getUserMedia error: ', error);
  videoLabel.innerHTML = "<p> Error Accessing video </p>";
}

// entry point for video
function videoStart(videoSource){
  var constraints = {
    video: {
      optional: [{sourceId: videoSource}]
    }
  };
  console.log('asking for videoSource', videoSource);
  console.log('asking for constraints', constraints);
  navigator.getUserMedia(constraints, videoSuccessCallback, errorCallback);
}

// end of video section
</script>


<script>
var videoElement = null;

// the render loop to move the video into texture
var animationFrame = function() {
  if (videoCount > 0) {
    if (!videoElement) {
      videoElement = document.querySelector('#video1');
    } else {
      if (videoElement.videoHeight) { // only defined when loaded
        gl.bindTexture(gl.TEXTURE_2D, texture);
        gl.pixelStorei(gl.UNPACK_FLIP_Y_WEBGL, true);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.LINEAR);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
        gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
        gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, videoElement);
        render3D();
      }
    }
  }
  requestAnimationFrame(animationFrame);
}
requestAnimationFrame(animationFrame);



</script>


<span id="orientation"></span>

<script>

//
// The orientation api provides an approximate orientation of the device, but
// may not be stable enough to use for tracking.  The X and Y rotations (with respect
// to the screen seem very quick and stable (gyro-based) but the Z rotation
// appears to be poor (based on accelerometer and magnetometer readings).
//


//
// from http://w3c.github.io/deviceorientation/spec-source-orientation.html#deviceorientation
//

var degtorad = Math.PI / 180; // Degree-to-Radian conversion

function getRotationMatrix( alpha, beta, gamma ) {

  var _x = beta  ? beta  * degtorad : 0; // beta value
  var _y = gamma ? gamma * degtorad : 0; // gamma value
  var _z = alpha ? alpha * degtorad : 0; // alpha value

  var cX = Math.cos( _x );
  var cY = Math.cos( _y );
  var cZ = Math.cos( _z );
  var sX = Math.sin( _x );
  var sY = Math.sin( _y );
  var sZ = Math.sin( _z );

  //
  // ZXY rotation matrix construction.
  //

  var m11 = cZ * cY - sZ * sX * sY;
  var m12 = - cX * sZ;
  var m13 = cY * sZ * sX + cZ * sY;

  var m21 = cY * sZ + cZ * sX * sY;
  var m22 = cZ * cX;
  var m23 = sZ * sY - cZ * cY * sX;

  var m31 = - cX * sY;
  var m32 = sX;
  var m33 = cX * cY;

  return [
    m11,    m12,    m13,
    m21,    m22,    m23,
    m31,    m32,    m33
  ];

};

function matrixTable(m) {
  var t = "<table>";
  for (row = 0; row < 3; row++) {
    t += "<tr>";
    for (column = 0; column < 3; column++) {
      t += "<td>" + m[3*row+column].toFixed(3) + "</td>";
    }
    t += "</tr>";
  }
  t += "</table>";
  return (t);
}

var orientationLabel = document.getElementById('orientation');
orientationLabel.innerHTML = "<p> waiting for orientation events...</p>";


var pendingAjax = undefined;
var currentMatrix = undefined;
var currentEulers = undefined;
var useEulers = true;

// used if sending rotation matrix
var sendTransform = function() {
  if (useEulers) {
    var action = "slicer/eulers?angles=" + String(currentEulers);
  } else {
    var action = "slicer/transform?m=" + String(currentMatrix);
  }

  pendingAjax = $.ajax(action)
  .done(function() {
    sendTransform();
  })
  .fail(function() {
    alert("Could not send transform, aborting");
  });
}

window.addEventListener('deviceorientation', function(e) {
        /* e.alpha, beta: e.beta, gamma: e.gamma */
        var orientationLabel = document.getElementById('orientation');
        orientationLabel.innerHTML = "<p> e = " + e + "</p>";
        orientationLabel.innerHTML += "<p> alpha = " + e.alpha + "</p>";
        orientationLabel.innerHTML += "<p> beta = " + e.beta + "</p>";
        orientationLabel.innerHTML += "<p> gamma = " + e.gamma + "</p>";
        orientationLabel.innerHTML += "<p> absolute = " + e.absolute + "</p>";

        currentEulers = [e.alpha, e.beta, e.gamma];

        currentMatrix = getRotationMatrix(e.alpha, e.beta, e.gamma);

        if (pendingAjax === undefined) {
          sendTransform();
        }

        orientationLabel.innerHTML += "<p> " + matrixTable(currentMatrix);

}, true);

// end of orientation section
</script>


<p>
This demo uses advanced web apis to access sensors.  Not all devices and browsers are supported.
</p>


</body>
</html>
